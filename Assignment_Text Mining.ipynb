{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a086b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import string\n",
    "import spacy\n",
    "from matplotlib.pyplot import imread\n",
    "\n",
    "# Text Mining\n",
    "from nltk.corpus import stopwords\n",
    "from textblob import TextBlob\n",
    "from wordcloud import WordCloud,STOPWORDS\n",
    "\n",
    "#warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "718b8a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = pd.read_csv(\"Elon_musk.csv\",encoding = 'latin')\n",
    "tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32157077",
   "metadata": {},
   "source": [
    "### 1. EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec8619c",
   "metadata": {},
   "source": [
    "#### 1.1 Number of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38604dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets['word_count'] = tweets['Text'].apply(lambda x: len(str(x).split(\" \")))\n",
    "tweets[['Text','word_count']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e225edb8",
   "metadata": {},
   "source": [
    "#### 1.2 Number of charachters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "942cbf16",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Number of characters in single tweet\n",
    "tweets['char_count'] = tweets['Text'].str.len() ## this also includes spaces\n",
    "tweets[['Text','char_count']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dac4308",
   "metadata": {},
   "source": [
    "#### 1.3 Average Word Length "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc08f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_word(sentence):\n",
    "    words = sentence.split()\n",
    "    return (sum(len(word) for word in words)/len(words))\n",
    "\n",
    "tweets['avg_word'] = tweets['Text'].apply(lambda x: avg_word(x))\n",
    "tweets[['Text','avg_word']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6190ee5e",
   "metadata": {},
   "source": [
    "#### 1.4 Number of stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb7c865",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = stopwords.words('english')\n",
    "\n",
    "tweets['stopwords'] = tweets['Text'].apply(lambda x: len([x for x in x.split() if x in stop]))\n",
    "tweets[['Text','stopwords']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10b6c98b",
   "metadata": {},
   "source": [
    "#### 1.5 Number of Special Characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c147cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets['hastags'] = tweets['Text'].apply(lambda x: len([x for x in x.split() if x.startswith('@')]))\n",
    "tweets[['Text','hastags']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be5c1eb",
   "metadata": {},
   "source": [
    "#### 1.6 Number of Numerics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c9b0b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets['numerics'] = tweets['Text'].apply(lambda x: len([x for x in x.split() if x.isdigit()]))\n",
    "tweets[['Text','numerics']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db950d29",
   "metadata": {},
   "source": [
    "#### 1.7 Number of Upper Case Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "173e0875",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets['upper'] = tweets['Text'].apply(lambda x: len([x for x in x.split() if x.isupper()]))\n",
    "tweets[['Text','upper']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b0f11a",
   "metadata": {},
   "source": [
    "###  2. Text-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf86ae5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets=[Text.strip() for Text in tweets.Text] # remove both the leading and the trailing characters\n",
    "tweets=[Text for Text in tweets if Text] # removes empty strings, because they are considered in Python as False\n",
    "tweets[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44576c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Joining the list into one string/text\n",
    "tweets_text=' '.join(tweets)\n",
    "tweets_text [0:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab5e6b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove Twitter username handles from a given twitter text. (Removes @usernames)\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tknzr = TweetTokenizer(strip_handles=True)\n",
    "tweets_tokens=tknzr.tokenize(tweets_text)\n",
    "print(tweets_tokens[0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ecf6ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Again Joining the list into one string/text\n",
    "tweets_tokens_text=' '.join(tweets_tokens)\n",
    "tweets_tokens_text [0:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d566bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Punctuations \n",
    "no_punc_text=tweets_tokens_text.translate(str.maketrans('','',string.punctuation))\n",
    "no_punc_text [0:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d8bfa29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove https or url within text\n",
    "import re\n",
    "no_url_text=re.sub(r'http\\S+', '', no_punc_text)\n",
    "no_url_text [0:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b07af15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "text_tokens=word_tokenize(no_url_text)\n",
    "print(text_tokens[0:150]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e168a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f7f4613",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokens count\n",
    "len(text_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ac9ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Stopwords\n",
    "from nltk.corpus import stopwords\n",
    "my_stop_words=stopwords.words('english')\n",
    "\n",
    "sw_list = ['\\x92','rt','ye','yeah','haha','Yes','U0001F923','I']\n",
    "my_stop_words.extend(sw_list)\n",
    "\n",
    "no_stop_tokens=[word for word in text_tokens if not word in my_stop_words]\n",
    "print(no_stop_tokens[0:150])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb1f6766",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the data\n",
    "lower_words=[Text.lower() for Text in no_stop_tokens]\n",
    "print(lower_words[100:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "854af418",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stemming (Optional)\n",
    "from nltk.stem import PorterStemmer\n",
    "ps=PorterStemmer()\n",
    "stemmed_tokens=[ps.stem(word) for word in lower_words]\n",
    "print(stemmed_tokens[100:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "070b50e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatization\n",
    "nlp=spacy.load('en_core_web_sm')\n",
    "doc=nlp(' '.join(lower_words))\n",
    "print(doc[0:150])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1427eccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmas=[token.lemma_ for token in doc]\n",
    "print(lemmas[0:150])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe8a2624",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_tweets=' '.join(lemmas)\n",
    "clean_tweets[0:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adee236c",
   "metadata": {},
   "source": [
    "### 3.Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5faa7e5f",
   "metadata": {},
   "source": [
    "#### 3.1 Using CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b5890fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv=CountVectorizer()\n",
    "tweetscv=cv.fit_transform(lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee1aab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cv.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d4f58e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cv.get_feature_names()[100:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea0cdc8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tweetscv.toarray()[100:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "247c2c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tweetscv.toarray().shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca908d2",
   "metadata": {},
   "source": [
    "#### 3.2 CountVectorizer with N-grams (Bigrams & Trigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "650bf294",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_ngram_range=CountVectorizer(analyzer='word',ngram_range=(1,3),max_features=100)\n",
    "bow_matrix_ngram=cv_ngram_range.fit_transform(lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83acd525",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cv_ngram_range.get_feature_names())\n",
    "print(bow_matrix_ngram.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf1ee1b",
   "metadata": {},
   "source": [
    "#### 3.3 TF-IDF Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e521f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidfv_ngram_max_features=TfidfVectorizer(norm='l2',analyzer='word',ngram_range=(1,3),max_features=500)\n",
    "tfidf_matix_ngram=tfidfv_ngram_max_features.fit_transform(lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed2e72c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tfidfv_ngram_max_features.get_feature_names())\n",
    "print(tfidf_matix_ngram.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aa96926",
   "metadata": {},
   "source": [
    "### 4. Generate Word Cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ac25e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to plot word cloud\n",
    "def plot_cloud(wordcloud):\n",
    "    plt.figure(figsize=(40,30))\n",
    "    plt.imshow(wordcloud)\n",
    "    plt.axis('off')\n",
    "    \n",
    "# Generate Word Cloud\n",
    "\n",
    "STOPWORDS.add('pron')\n",
    "STOPWORDS.add('rt')\n",
    "STOPWORDS.add('yeah')\n",
    "wordcloud=WordCloud(width=3000,height=2000,background_color='black',max_words=50,\n",
    "                   colormap='Set1',stopwords=STOPWORDS).generate(clean_tweets)\n",
    "plot_cloud(wordcloud)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0116c9bd",
   "metadata": {},
   "source": [
    "### 5. Named Entity Recognition (NER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a40272fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parts Of Speech (POS) Tagging\n",
    "nlp=spacy.load('en_core_web_sm')\n",
    "\n",
    "one_block=clean_tweets\n",
    "doc_block=nlp(one_block)\n",
    "spacy.displacy.render(doc_block,style='ent',jupyter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd22d3ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in doc_block[100:200]:\n",
    "    print(token,token.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7047616",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering the nouns and verbs only\n",
    "nouns_verbs=[token.text for token in doc_block if token.pos_ in ('NOUN','VERB')]\n",
    "print(nouns_verbs[100:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f40964ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counting the noun & verb tokens\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv=CountVectorizer()\n",
    "\n",
    "X=cv.fit_transform(nouns_verbs)\n",
    "sum_words=X.sum(axis=0)\n",
    "\n",
    "words_freq=[(word,sum_words[0,idx]) for word,idx in cv.vocabulary_.items()]\n",
    "words_freq=sorted(words_freq, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "wd_df=pd.DataFrame(words_freq)\n",
    "wd_df.columns=['word','count']\n",
    "wd_df[0:10] # viewing top ten results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "281295d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing results (Barchart for top 10 nouns + verbs)\n",
    "wd_df[0:10].plot.bar(x='word',figsize=(12,8),title='Top 10 nouns and verbs');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d695d755",
   "metadata": {},
   "source": [
    "### 6. Emotion Mining - Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103d313c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import tokenize\n",
    "sentences=tokenize.sent_tokenize(' '.join(tweets))\n",
    "print(sentences[0:150])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e21a4a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_df=pd.DataFrame(sentences,columns=['sentence'])\n",
    "sent_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "483c6d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Emotion Lexicon - Affin\n",
    "affin=pd.read_csv('Afinn.csv',sep=',',encoding='Latin-1')\n",
    "affin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0456ef8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "affinity_scores=affin.set_index('word')['value'].to_dict()\n",
    "affinity_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a613730",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom function: score each word in a sentence in lemmatised form, but calculate the score for the whole original sentence\n",
    "nlp=spacy.load('en_core_web_sm')\n",
    "sentiment_lexicon=affinity_scores\n",
    "\n",
    "def calculate_sentiment(text:str=None):\n",
    "    sent_score=0\n",
    "    if text:\n",
    "        sentence=nlp(text)\n",
    "        for word in sentence:\n",
    "            sent_score+=sentiment_lexicon.get(word.lemma_,0)\n",
    "    return sent_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b860193a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# manual testing\n",
    "calculate_sentiment(text='great')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1039fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating sentiment value for each sentence\n",
    "sent_df['sentiment_value']=sent_df['sentence'].apply(calculate_sentiment)\n",
    "sent_df['sentiment_value']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e00881",
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many words are there in a sentence?\n",
    "sent_df['word_count']=sent_df['sentence'].str.split().apply(len)\n",
    "sent_df['word_count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "206ee4d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_df.sort_values(by='sentiment_value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0400e299",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment score of the whole review\n",
    "sent_df['sentiment_value'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc05bde4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# negative sentiment score of the whole review\n",
    "sent_df[sent_df['sentiment_value']<=0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "805c9bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding index cloumn\n",
    "sent_df['index']=range(0,len(sent_df))# Plotting the line plot for sentiment value of whole review\n",
    "plt.figure(figsize=(15,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb7bfb03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the sentiment value for whole review\n",
    "\n",
    "plt.figure(figsize=(15,10))\n",
    "sns.distplot(sent_df['sentiment_value'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eceb8326",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the line plot for sentiment value of whole review\n",
    "plt.figure(figsize=(15,10))\n",
    "sns.lineplot(y='sentiment_value',x='index',data=sent_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cefb2034",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation analysis\n",
    "sent_df.plot.scatter(x='word_count',y='sentiment_value',figsize=(8,8),title='Sentence sentiment value to sentence word count')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
